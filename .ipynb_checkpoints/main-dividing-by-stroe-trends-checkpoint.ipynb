{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b491ffaf",
   "metadata": {},
   "source": [
    "----\n",
    "# I. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2034d",
   "metadata": {},
   "source": [
    "## 1. Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "128e1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"dataset/train.csv\")\n",
    "test = pd.read_csv(\"dataset/test.csv\")\n",
    "submission = pd.read_csv(\"dataset/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdbfaac",
   "metadata": {},
   "source": [
    "## 2. Missing values handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8a773",
   "metadata": {},
   "source": [
    "### a) `train.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2685bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def means(df):\n",
    "    res = {}\n",
    "    for i in range(1, 6):\n",
    "        res[f\"Promotion{i}\"] = df[f\"Promotion{i}\"].mean()\n",
    "    return res\n",
    "\n",
    "\n",
    "def medians(df):\n",
    "    res = {}\n",
    "    for i in range(1, 6):\n",
    "        res[f\"Promotion{i}\"] = df[f\"Promotion{i}\"].median()\n",
    "    return res\n",
    "\n",
    "\n",
    "train = train.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c34093",
   "metadata": {},
   "source": [
    "### b) `test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d62391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(value=means(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ffd826",
   "metadata": {},
   "source": [
    "## 3. Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faea470",
   "metadata": {},
   "source": [
    "### a) `Date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0243e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def date_to_week(date):\n",
    "    day, month, year = map(int, date.split('/'))\n",
    "    t = dt.datetime(year, month, day) - dt.datetime(2010, 2, 5)\n",
    "    return int(t.days // 7)\n",
    "\n",
    "def date_split(date):\n",
    "    return tuple(map(int, date.split('/')))\n",
    "\n",
    "\n",
    "# train\n",
    "train['Week'] = train['Date'].apply(date_to_week)\n",
    "train[\"Day\"] = train[\"Date\"].apply(lambda x: date_split(x)[0])\n",
    "train[\"Month\"] = train[\"Date\"].apply(lambda x: date_split(x)[1])\n",
    "train[\"Year\"] = train[\"Date\"].apply(lambda x: date_split(x)[2])\n",
    "\n",
    "\n",
    "# test\n",
    "test['Week'] = test['Date'].apply(date_to_week)\n",
    "test[\"Day\"] = test[\"Date\"].apply(lambda x: date_split(x)[0])\n",
    "test[\"Month\"] = test[\"Date\"].apply(lambda x: date_split(x)[1])\n",
    "test[\"Year\"] = test[\"Date\"].apply(lambda x: date_split(x)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7d458",
   "metadata": {},
   "source": [
    "### b) `IsHoliday`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "515e1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trian\n",
    "train[\"IsHoliday\"] = train[\"IsHoliday\"].apply(int)\n",
    "\n",
    "# test\n",
    "test[\"IsHoliday\"] = test[\"IsHoliday\"].apply(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62e66f",
   "metadata": {},
   "source": [
    "### c) `Promotion1`, ... , `Promotion5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e0daf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "promos = ['Promotion1','Promotion2','Promotion3','Promotion4','Promotion5']\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(train[promos])\n",
    "train[promos] = scaler.transform(train[promos])\n",
    "test[promos] = scaler.transform(test[promos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9f1dc",
   "metadata": {},
   "source": [
    "### d) Data Scaling to all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23be48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "train[\"Weekly_Sales\"] = np.log1p(train[\"Weekly_Sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85389a7",
   "metadata": {},
   "source": [
    "## 4. Remove not using features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df8eb49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "removes = ['id','Date','Temperature'] \n",
    "\n",
    "train = train.drop(columns=removes)\n",
    "test = test.drop(columns=removes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667509e",
   "metadata": {},
   "source": [
    "## 5. Divide by Store-trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ff7ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(df, stores):\n",
    "    divided = pd.DataFrame()\n",
    "    for store in stores:\n",
    "        divided = pd.concat([divided, df[df[\"Store\"] == store].copy()])\n",
    "    return divided\n",
    "\n",
    "\n",
    "def remove(df, stores):\n",
    "    for store in stores:\n",
    "        df = df.drop(df[df[\"Store\"] == store].index)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def store_one_hot(df):\n",
    "    return pd.get_dummies(data=df, columns=[\"Store\"])\n",
    "#####################################\n",
    "\n",
    "trainA = divide(train, [36])\n",
    "train = remove(train, [36])\n",
    "\n",
    "testA = divide(test, [36])\n",
    "test = remove(test, [36])\n",
    "\n",
    "#\n",
    "\n",
    "trainB = divide(train, [38])\n",
    "train = remove(train, [38])\n",
    "\n",
    "testB = divide(test, [38])\n",
    "test = remove(test, [38])\n",
    "\n",
    "#\n",
    "\n",
    "trainC = divide(train, [33])\n",
    "train = remove(train, [33])\n",
    "\n",
    "testC = divide(test, [33])\n",
    "test = remove(test, [33])\n",
    "\n",
    "#\n",
    "\n",
    "trainD = divide(train, [42])\n",
    "train = remove(train, [42])\n",
    "\n",
    "testD = divide(test, [42])\n",
    "test = remove(test, [42])\n",
    "\n",
    "#\n",
    "\n",
    "trainE = divide(train, [43])\n",
    "train = remove(train, [43])\n",
    "\n",
    "testE = divide(test, [43])\n",
    "test = remove(test, [43])\n",
    "\n",
    "#\n",
    "\n",
    "trainF = divide(train, [44])\n",
    "train = remove(train, [44])\n",
    "\n",
    "testF = divide(test, [44])\n",
    "test = remove(test, [44])\n",
    "\n",
    "#################################################################\n",
    "\n",
    "trainA = trainA[['Day','Month','Week','Weekly_Sales']]\n",
    "testA = testA[['Day','Month','Week']]\n",
    "\n",
    "trainB = trainB[['Day','Month','Week','Weekly_Sales','IsHoliday']]\n",
    "testB = testB[['Day','Month','Week','IsHoliday']]\n",
    "\n",
    "\n",
    "trainC = trainC[['Day','Month','Week','Weekly_Sales','IsHoliday']]\n",
    "testC = testC[['Day','Month','Week','IsHoliday']]\n",
    "\n",
    "\n",
    "trainD = trainD[['Day','Month','Week','Weekly_Sales','IsHoliday']]\n",
    "testD = testD[['Day','Month','Week','IsHoliday']]\n",
    "\n",
    "\n",
    "trainE = trainE[['Day','Month','Week','Weekly_Sales','IsHoliday']]\n",
    "testE = testE[['Day','Month','Week','IsHoliday']]\n",
    "\n",
    "\n",
    "trainF = trainF[['Day','Month','Week','Weekly_Sales','IsHoliday']]\n",
    "testF = testF[['Day','Month','Week','IsHoliday']]\n",
    "#################################################################\n",
    "\n",
    "train = store_one_hot(train)\n",
    "test = store_one_hot(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4806e6df",
   "metadata": {},
   "source": [
    "----\n",
    "# II. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f106b",
   "metadata": {},
   "source": [
    "## 1. Divide `train.csv` into training data and for predicting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78b4e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pred(df):\n",
    "    x_df = df.drop(columns=[\"Weekly_Sales\"])\n",
    "    y_df = df[\"Weekly_Sales\"]\n",
    "    return x_df, y_df\n",
    "    \n",
    "\n",
    "x_train, y_train = split_pred(train)\n",
    "x_trainA, y_trainA = split_pred(trainA)\n",
    "x_trainB, y_trainB = split_pred(trainB)\n",
    "x_trainC, y_trainC = split_pred(trainC)\n",
    "x_trainD, y_trainD = split_pred(trainD)\n",
    "x_trainE, y_trainE = split_pred(trainE)\n",
    "x_trainF, y_trainF = split_pred(trainF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9b2dc",
   "metadata": {},
   "source": [
    "## 2. Modeling for each stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74027679",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd684312",
   "metadata": {},
   "source": [
    "### a) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d80b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290243.55912224 285468.64658678 280772.28777095 276153.19036063]\n",
      "[465268.39673659 449458.90492457 434186.60733401 419433.25052358]\n",
      "[262282.03 279813.06 258359.08 243765.47]\n",
      "[596982.9  597954.4  528412.94 511566.62]\n",
      "[637221.94 630554.1  618927.4  588358.44]\n",
      "[350392.9  346539.66 341187.88 349898.03]\n",
      "[XGBoost] => time: 2.26(sec)\n",
      "1658464.75, 1550350.25, ... , 769050.125\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression as linear\n",
    "\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror', \n",
    "    learning_rate=0.1, \n",
    "    max_depth = 4, \n",
    "    n_estimators = 1000\n",
    ")\n",
    "model.fit(x_train, y_train)\n",
    "pred = np.expm1(model.predict(test))\n",
    "\n",
    "\n",
    "def unify(pred, members):\n",
    "    for i, k in enumerate(members):\n",
    "        y_pred[(k-1)*4:k*4] = pred[i*4:(i+1)*4]\n",
    "        \n",
    "\n",
    "model = linear()\n",
    "model.fit(x_trainA, y_trainA)\n",
    "predA = np.expm1(model.predict(testA))\n",
    "print(predA)\n",
    "\n",
    "model = linear()\n",
    "model.fit(x_trainB, y_trainB)\n",
    "predB = np.expm1(model.predict(testB))\n",
    "print(predB)\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(x_trainC, y_trainC)\n",
    "predC = np.expm1(model.predict(testC))\n",
    "print(predC)\n",
    "\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(x_trainD, y_trainD)\n",
    "predD = np.expm1(model.predict(testD))\n",
    "print(predD)\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(x_trainE, y_trainE)\n",
    "predE = np.expm1(model.predict(testE))\n",
    "print(predE)\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(x_trainF, y_trainF)\n",
    "predF = np.expm1(model.predict(testF))\n",
    "print(predF)\n",
    "\n",
    "\n",
    "y_pred = [0] * 180\n",
    "unify(predA, [36])\n",
    "unify(predB, [38])\n",
    "unify(predC, [33])\n",
    "unify(predD, [42])\n",
    "unify(predE, [43])\n",
    "unify(predF, [44])\n",
    "\n",
    "\n",
    "\n",
    "j = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        y_pred[i] = pred[j]\n",
    "        j += 1\n",
    "\n",
    "predictions[\"XGBoost\"] = y_pred\n",
    "\n",
    "print(f\"[XGBoost] => time: {round(time.time() - start_t, 2)}(sec)\")\n",
    "print(f\"{y_pred[0]}, {y_pred[1]}, ... , {y_pred[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a880c",
   "metadata": {},
   "source": [
    "----\n",
    "# III. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cf6f200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done!] Create 1 csv file(s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def name(integer):\n",
    "    return str(integer).zfill(2)\n",
    "\n",
    "\n",
    "savetime = dt.datetime.now()\n",
    "folder = \"-\".join(map(name, [savetime.year, savetime.month, savetime.day]))\n",
    "sub_folder = name(savetime.hour) + '：' + name(savetime.minute) + '：' + name(savetime.second)\n",
    "\n",
    "for model in predictions:\n",
    "    submission[\"Weekly_Sales\"] = predictions[model]\n",
    "    os.makedirs(f\"dataset/submissions/{folder}\", exist_ok=True)\n",
    "    submission.to_csv(f\"dataset/submissions/{folder}/{sub_folder+' ('+model+')'}.csv\", index=False)\n",
    "print(f\"[Done!] Create {len(predictions)} csv file(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af93dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
